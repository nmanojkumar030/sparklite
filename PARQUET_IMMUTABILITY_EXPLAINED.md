# Why Parquet Row Groups and Files Are Immutable

## üîç **Technical Architecture Analysis**

### **1. Footer-Based Metadata Design**

**Source: [Apache Parquet Official Format Specification](https://parquet.apache.org/docs/file-format/)**

```
4-byte magic number "PAR1"
<Column 1 Chunk 1>
<Column 2 Chunk 1>
...
<Column N Chunk 1>
<Column 1 Chunk 2>
<Column 2 Chunk 2>
...
<Column N Chunk 2>
...
<Column 1 Chunk M>
<Column 2 Chunk M>
...
<Column N Chunk M>
File Metadata
4-byte length in bytes of file metadata (little endian)
4-byte magic number "PAR1"
```

**Why This Makes Files Immutable:**

1. **Metadata Location**: All file metadata is stored at the **end** of the file (footer)
2. **Single-Pass Writing**: "File metadata is written after the data to allow for single pass writing" - *Apache Parquet Specification*
3. **Block Boundaries**: Row group locations are stored in footer metadata, not as headers
4. **Schema Information**: Complete schema definition is in the footer

**Implication**: To add a new row group, you would need to:
- Rewrite the entire footer metadata
- Update all row group location pointers
- Potentially invalidate existing readers mid-stream

---

### **2. Columnar Storage Architecture**

**Source: [Apache Parquet Format Repository](https://github.com/apache/parquet-format)**

**Row Group Structure:**
```
Row Group = Collection of Column Chunks
Column Chunk = Collection of Pages (compressed as unit)
Page = Smallest unit of compression/encoding
```

**Why Row Groups Are Immutable:**

1. **Compression Units**: "Column chunks are compressed as units" - *Parquet Specification*
2. **Page Organization**: "A page is conceptually an indivisible unit (in terms of compression and encoding)" - *Apache Parquet Docs*
3. **Columnar Layout**: Data is organized by column, not row - cannot insert individual records

**Technical Reality:**
```java
// This is IMPOSSIBLE in Parquet:
rowGroup.addRecord(newRecord);  // ‚ùå Cannot modify existing row group

// This is how Parquet works:
List<Record> completeRowGroup = collectAllRecords();
writeCompleteRowGroup(completeRowGroup);  // ‚úÖ Write entire row group at once
```

---

### **3. Dremel Encoding Constraints**

**Source: [Google Dremel Paper](https://research.google.com/pubs/pub36632.html) & [Parquet Nested Encoding](https://github.com/apache/parquet-format/blob/master/README.md#nested-encoding)**

**Nested Data Encoding:**
- **Definition Levels**: Specify how many optional fields are defined
- **Repetition Levels**: Specify at what repeated field the value is repeated
- **Run-Length Encoding**: Used for efficient level storage

**Why This Prevents Modification:**
```
Definition Levels: [2, 2, 1, 2, 2, 0, 2, 2]  // Pre-calculated for entire column
Repetition Levels: [0, 1, 0, 0, 1, 0, 0, 1]  // Pre-calculated for entire column
Values:           [A, B, null, C, D, null, E, F]
```

**Inserting a single record would require:**
1. Recalculating all definition/repetition levels
2. Re-encoding the entire column chunk
3. Updating compression boundaries
4. Rewriting metadata

---

### **4. Industry Expert Validation**

**Source: [Dremio - Tuning Parquet Performance](https://www.dremio.com/blog/tuning-parquet/)**

> "Parquet is built to be used by anyone... an efficient, well-implemented columnar storage substrate should be useful to all frameworks without the cost of extensive and difficult to set up dependencies."

**Key Quote on Immutability:**
> "Row groups are the primary unit of storage and processing in Parquet files. Each row group contains data for a subset of rows, stored in column chunks."

**Source: [CelerData - Parquet File Format Guide](https://celerdata.com/glossary/parquet-file-format)**

> "Parquet's immutable design: Optimized for analytical workloads, not OLTP"

**Technical Explanation:**
- **Write-Once, Read-Many**: Parquet optimized for analytical queries
- **Compression Efficiency**: Requires complete column chunks for optimal compression
- **Statistics Generation**: Min/max statistics calculated per row group

---

### **5. Comparison with Mutable Formats**

| **Aspect** | **Parquet (Immutable)** | **Traditional RDBMS (Mutable)** |
|------------|-------------------------|----------------------------------|
| **Metadata Location** | Footer (end of file) | Header + distributed |
| **Write Pattern** | Batch/Bulk writes | Individual record writes |
| **Compression** | Column-level, high ratio | Row-level, lower ratio |
| **Update Support** | None (create new files) | In-place updates |
| **Query Performance** | Excellent for analytics | Good for OLTP |

---

### **6. Practical Implications**

**What You CANNOT Do:**
```java
// ‚ùå These operations are impossible:
parquetFile.appendRowGroup(newRowGroup);
parquetFile.updateRecord(recordId, newValues);
rowGroup.addRecord(record);
rowGroup.deleteRecord(recordId);
```

**What You MUST Do Instead:**
```java
// ‚úÖ Buffer until complete row group
RowGroupBuffer buffer = new RowGroupBuffer();
buffer.addRecord(record1);
buffer.addRecord(record2);
// ... continue until buffer is full (128MB)

if (buffer.isFull()) {
    String newFilePath = generateVersionedPath();
    writeCompleteFile(newFilePath, buffer.getAllRecords());
    buffer.clear();
}
```

---

### **7. Design Philosophy**

**Source: [Apache Parquet Motivation](https://parquet.apache.org/docs/overview/motivation/)**

> "We created Parquet to make the advantages of compressed, efficient columnar data representation available to any project in the Hadoop ecosystem."

**Core Design Principles:**
1. **Analytical Optimization**: Designed for read-heavy analytical workloads
2. **Compression First**: Maximize compression through columnar storage
3. **Schema Evolution**: Support schema changes without breaking existing files
4. **Cross-Platform**: Language and framework agnostic

**Why Immutability Supports These Goals:**
- **Compression**: Complete column chunks compress better than partial chunks
- **Performance**: Predictable I/O patterns for analytical queries
- **Reliability**: No risk of corruption from partial writes
- **Simplicity**: Simpler reader/writer implementations

---

## üéØ **Summary: The Immutability Reality**

### **Technical Reasons:**
1. **Footer Metadata**: All structural information stored at file end
2. **Columnar Compression**: Column chunks compressed as complete units
3. **Dremel Encoding**: Definition/repetition levels calculated for entire columns
4. **Page Structure**: Pages are indivisible compression/encoding units

### **Design Philosophy:**
1. **Analytical Focus**: Optimized for read-heavy, not write-heavy workloads
2. **Compression Priority**: Maximum storage efficiency through complete column chunks
3. **Simplicity**: Easier to implement and maintain than mutable formats

### **Practical Solution:**
```java
// Instead of modifying existing files, use versioning:
public class ParquetStorageStrategy {
    public void handleIncrementalWrites() {
        // Buffer records until row group size reached
        while (hasMoreRecords()) {
            buffer.add(nextRecord());
            
            if (buffer.reachedThreshold()) {
                String versionedPath = generateNewFilePath();
                writeCompleteParquetFile(versionedPath, buffer);
                buffer.clear();
            }
        }
    }
}
```

**Bottom Line**: Parquet's immutability is not a limitation‚Äîit's a fundamental design choice that enables its exceptional performance for analytical workloads. The format trades write flexibility for read performance, compression efficiency, and implementation simplicity. 