
# TODO.md â€” Build a Miniature Spark in Java

This project is a step-by-step implementation of a Spark-like distributed data processing engine in Java, inspired by Apache Spark's RDD architecture. We will follow Test-Driven Development (TDD) principles throughout.

---

## âœ… Phase 1: RDD Core Abstractions (Local Execution)

### â˜ Step 1: Create `MiniRDD<T>`
- [ ] Abstract base class with `map`, `filter`, and `collect()` methods.
- [ ] `.map()` and `.filter()` should return new `MiniRDD` instances that keep a reference to the parent RDD.
- [ ] `.collect()` should evaluate the entire lineage lazily.

### â˜ Step 2: Implement `ParallelCollectionRDD<T>`
- [ ] Create a concrete subclass of `MiniRDD` that wraps a static list.
- [ ] Implements `collect()` by returning its internal data.

### â˜ Step 3: Implement `MapRDD<T, U>` and `FilterRDD<T>`
- [ ] Apply transformation on parent data in `collect()`.
- [ ] Must call `parent.collect()` internally.

### â˜ Step 4: Write Unit Tests
- [ ] `testMapAndFilter()`
- [ ] `testEmptyRDD()`
- [ ] Test transformation chaining (e.g., `rdd.filter(...).map(...).filter(...)`)

---

## ğŸ” Phase 2: Partitioning and Parallelism

### â˜ Step 5: Introduce `Partition` abstraction
- [ ] Create `MiniPartition` class with `partitionId` and data slice.
- [ ] Add `MiniRDD.getPartitions()` returning list of partitions.

### â˜ Step 6: Implement `mapPartitions()` method
- [ ] Add to `MiniRDD<T>` to support transformation on partition level.

### â˜ Step 7: Execute partitions in parallel
- [ ] Create `LocalScheduler` that executes one task per partition using a thread pool.
- [ ] `MiniRDD.collect()` should submit partition tasks to the scheduler.

### â˜ Step 8: Write tests for parallel execution
- [ ] Use timing or thread ID tracking to verify concurrency.

---

## ğŸ§± Phase 3: DAG Scheduler and Stages

### â˜ Step 9: Create `Stage` and `Task` classes
- [ ] A `Stage` represents a group of `Task`s with no shuffle boundaries.
- [ ] A `Task` processes a single partition.

### â˜ Step 10: Implement `DAGScheduler`
- [ ] Resolves RDD lineage into a DAG of stages.
- [ ] Submits ready stages to the task scheduler in order.

### â˜ Step 11: Add fault injection and retries
- [ ] Simulate failure in a task and re-run from parent RDD.
- [ ] Write tests to confirm retry behavior.

---

## ğŸŒ‰ Phase 4: Shuffle and Wide Dependencies

### â˜ Step 12: Implement `groupByKey()` with simulated shuffle
- [ ] Partition values by key hash and regroup on new RDD.
- [ ] Add `ShuffleMapStage` and `ResultStage` in DAG.

### â˜ Step 13: Simulate shuffle file writing and reading
- [ ] Write partitioned output to temp storage.
- [ ] Later stages read partitioned input from shuffle files.

---

## ğŸ§­ Phase 5: Remote Execution Simulation (Optional)

### â˜ Step 14: Add `Executor` and `TaskScheduler`
- [ ] Simulate Spark's cluster mode locally.
- [ ] Use threads or lightweight RPC to simulate remote workers.

### â˜ Step 15: Add driver/executor message protocol
- [ ] Submit task from driver to executor.
- [ ] Collect result back into driver.

---

## ğŸ§ª Testing Strategy

- [ ] Every transformation should have unit tests.
- [ ] Every scheduler should have integration tests.
- [ ] Add fault injection tests for retries and lineage recomputation.

---

## ğŸ“¦ Suggested Project Structure

```
minispark/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ MiniRDD.java
â”‚   â”œâ”€â”€ ParallelCollectionRDD.java
â”‚   â””â”€â”€ transformations/
â”‚       â”œâ”€â”€ MapRDD.java
â”‚       â””â”€â”€ FilterRDD.java
â”œâ”€â”€ scheduler/
â”‚   â”œâ”€â”€ Task.java
â”‚   â”œâ”€â”€ Stage.java
â”‚   â””â”€â”€ DAGScheduler.java
â”œâ”€â”€ executor/
â”‚   â”œâ”€â”€ LocalExecutor.java
â”‚   â””â”€â”€ TaskScheduler.java
â”œâ”€â”€ test/
â”‚   â””â”€â”€ MiniRDDTest.java
â”œâ”€â”€ build.gradle
â””â”€â”€ TODO.md
```

---

## ğŸ“ Notes

- This project uses **Java + JUnit 5 + Gradle**.
- Code structure and concepts should match Apache Spark as closely as possible.
- Later phases may evolve toward Spark SQL's physical plan structure.
